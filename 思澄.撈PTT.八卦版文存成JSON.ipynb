{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 目標\n",
    "請撰寫一個程式來上網抓取ptt 八卦版的資料\n",
    "網址: https://www.ptt.cc/bbs/Gossiping/index.html\n",
    "撰寫程式語言不拘(建議用python)\n",
    "對於網路爬蟲可以使用 jsoup 或是 beautiful soup \n",
    "要爬取的資料有 :\n",
    "文章的作者ID、標題、內文、留言\n",
    "共爬取40篇文章\n",
    "將資料全部包成一個json格式的檔案"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "問題:\n",
    "1. 特殊格式的文章\n",
    " - [公告] 水桶 之類的\n",
    "2. 果然還是要先分析 目標 \n",
    " - 找到 要抓取的內容\n",
    " - 選擇分析方式"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "參考資料:\n",
    "1. BeautifulSoup\n",
    " - https://www.crummy.com/software/BeautifulSoup/bs4/doc/\n",
    "- ptt-web-crawler (PTT 網路版爬蟲) \n",
    " - https://github.com/jwlin/ptt-web-crawler\\\n",
    " - https://www.ptt.cc/bbs/Python/M.1484321622.A.951.html\n",
    "- 瞭解JSON格式\n",
    " - http://j796160836.pixnet.net/blog/post/30530326-%E7%9E%AD%E8%A7%A3json%E6%A0%BC%E5%BC%8F\n",
    "- JSON UTF-8 格式儲存\n",
    " - https://stackoverflow.com/questions/18337407/saving-utf-8-texts-in-json-dumps-as-utf8-not-as-u-escape-sequence\n",
    "- JSON 格式轉換 網站\n",
    " - http://jsoneditoronline.org/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from six import u\n",
    "import codecs\n",
    "#import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 目標網頁"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "webLoca = \"https://www.ptt.cc\"\n",
    "targetWeb = '/bbs/Gossiping/index.html'\n",
    "fileNameJSON = 'output.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 函數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def FindContent(soup, article_id):\n",
    "    main_content = soup.find(id=\"main-content\")\n",
    "    # 移除 '※ 發信站:' (starts with u'\\u203b'), '◆ From:' (starts with u'\\u25c6'), 空行及多餘空白\n",
    "    # 保留英數字, 中文及中文標點, 網址, 部分特殊符號\n",
    "    filtered = [ v for v in main_content.stripped_strings if v[0] not in [u'※', u'◆'] and v[:2] not in [u'--'] ]\n",
    "    expr = re.compile(u(r'[^\\u4e00-\\u9fa5\\u3002\\uff1b\\uff0c\\uff1a\\u201c\\u201d\\uff08\\uff09\\u3001\\uff1f\\u300a\\u300b\\s\\w:/-_.?~%()]'))\n",
    "    for i in range(len(filtered)):\n",
    "        filtered[i] = re.sub(expr, '', filtered[i])\n",
    "\n",
    "    filtered = [_f for _f in filtered if _f]  # remove empty strings\n",
    "    #filtered = [x for x in filtered if article_id not in x]  # remove last line containing the url of the article\n",
    "    filtered_TEMP = []  # remove last line containing the url of the article\n",
    "    count = 0\n",
    "    for x in filtered :\n",
    "        count+=1\n",
    "        if count <=8: #切頭，文章訊息\n",
    "            continue\n",
    "        if article_id not in x: #斷尾，去掉文章連結後的留言\n",
    "            filtered_TEMP.append(x)\n",
    "            continue\n",
    "    #    if count >= 0:\n",
    "        break\n",
    "    #    count +=1\n",
    "    content = ' '.join(filtered_TEMP)\n",
    "    content = re.sub(r'(\\s)+', ' ', content)\n",
    "    #print ('content:', content)\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "def GetSpecifiedData_oneArticle(articleDivBlock):\n",
    "    try:\n",
    "        #print(webLoca + articleDivBlock.select('a')[0]['href'])\n",
    "        articleRES = rs.get(webLoca  + '/' + articleDivBlock.select('a')[0]['href'], verify=False)\n",
    "        articleSoup = BeautifulSoup(articleRES.text, 'lxml')\n",
    "        #作者與標題\n",
    "        try: \n",
    "            author = articleSoup.select('.article-meta-value')[0].text #作者ID\n",
    "            title = articleSoup.select('.article-meta-value')[2].text #標題\n",
    "            #print(author, title)\n",
    "        except IndexError as err: #發生於 [公告] 12月八卦板置底閒聊文，沒有作者與標題\n",
    "            #print (err, '公告文')\n",
    "            author = None\n",
    "            #title = None\n",
    "            title = \"公告\"\n",
    "            pass\n",
    "        #內文\n",
    "        try: \n",
    "            #print (articleSoup.select('#main-container'))\n",
    "            article_id = articleDivBlock.select('a')[0]['href'].split('/')[-1]\n",
    "            content = FindContent(articleSoup, re.sub('\\.html', '', article_id))\n",
    "            #print(content)\n",
    "        except IndexError as err: #\n",
    "            #print (err, '沒內文?')\n",
    "            content = None\n",
    "            #pass\n",
    "        #留言\n",
    "        push_content_list = []\n",
    "        try: \n",
    "            for pushMsg in articleSoup.select('.push'):\n",
    "                #push_userid = pushMsg.select('span[class^=f3]')[0].text #留言者\n",
    "                push_content = pushMsg.select('span[class^=f3]')[1].text.split(':',1)[1] #留言\n",
    "                push_content_list.append(push_content)\n",
    "                #print(push_userid, push_content)\n",
    "                #break\n",
    "        except IndexError as err: #發生於沒有推文 #這好像沒用\n",
    "            #print (err, '沒有推文')\n",
    "            #push_userid = None\n",
    "            push_content_list = None\n",
    "            #pass\n",
    "\n",
    "        #print(author, title)\n",
    "        #print(content, push_content_list)\n",
    "    except IndexError as err: #本文已刪除，無連結可進\n",
    "        #print (err, '本文已刪除')\n",
    "        title = articleDivBlock.select('.title')[0].text\n",
    "        author = articleDivBlock.select('.author')[0].text\n",
    "        content = None\n",
    "        push_content_list = None\n",
    "    \n",
    "    data = {\"author\":author,\n",
    "            \"title\":title,\n",
    "            \"content\":content,\n",
    "            \"push_content_list\": push_content_list\n",
    "            }\n",
    "    #time.sleep(0.1)\n",
    "    return json.dumps(data, sort_keys=True, ensure_ascii=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def StoreJSON(data, mode):\n",
    "    global fileNameJSON\n",
    "    with codecs.open(fileNameJSON, mode, encoding='utf-8') as f:\n",
    "        f.write(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "######  已滿18歲"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "payload = { 'from': targetWeb, \n",
    "           'yes':'yes'\n",
    "          }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rs = requests.session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "res = rs.post(\"https://www.ptt.cc/ask/over18\", verify = False, data = payload)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 進入文章列表"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "res = rs.get(webLoca + targetWeb, verify = False)\n",
    "soup = BeautifulSoup(res.text, 'lxml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 撈取資料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#dataList = []\n",
    "countArticleNum = 0\n",
    "targetArticleNum = 40\n",
    "chatArticleTF = True #跳過公告用\n",
    "firstTF = True #辨認頭一組內容"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#開頭\n",
    "StoreJSON('{','w')\n",
    "\n",
    "res = rs.get(webLoca + targetWeb, verify = False)\n",
    "soup = BeautifulSoup(res.text, 'lxml')\n",
    "while countArticleNum < targetArticleNum:\n",
    "    #print(webLoca +soup.select('a[class^=btn]')[3]['href'])\n",
    "    for articleDivBlock in reversed(soup.select('.r-ent')): #從底記上來，越底越新 ？\n",
    "        if chatArticleTF: #略過公告\n",
    "            chatArticleTF=False\n",
    "            continue\n",
    "        countArticleNum += 1\n",
    "        if firstTF: #辨認頭一組內容\n",
    "            StoreJSON('\\n' + '\"'+str(countArticleNum)+'\":[','a')\n",
    "            firstTF=False\n",
    "        else:\n",
    "            StoreJSON(',\\n' + '\"'+str(countArticleNum)+'\":[','a')\n",
    "        data = GetSpecifiedData_oneArticle(articleDivBlock)\n",
    "        StoreJSON(data,'a')\n",
    "        StoreJSON(']','a')\n",
    "        if countArticleNum >= targetArticleNum:\n",
    "            break\n",
    "    #換下一頁\n",
    "    res = rs.get(webLoca + soup.select('a[class^=btn]')[3]['href'], verify=False)\n",
    "    soup = BeautifulSoup(res.text, 'lxml')\n",
    "            \n",
    "#結尾\n",
    "StoreJSON('\\n}','a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 儲存輸出\n",
    "輸出函數已寫在上面"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "try:\n",
    "    with open('outputJSON.txt', \"w\", encoding='utf8') as outputFile:\n",
    "        json.dump(data, outputFile, ensure_ascii=False)\n",
    "except IOError as err:\n",
    "    print('File error:', str(err))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 顯示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getJSON(filename, mode='r'):\n",
    "    with codecs.open(filename, mode, encoding='utf-8') as f:\n",
    "        return json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print (json.dumps(getJSON(fileNameJSON), indent=4, sort_keys=True,ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
